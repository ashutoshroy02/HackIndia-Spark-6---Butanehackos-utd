{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-build-core==0.9.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfYmYaTDb_iz",
        "outputId": "93a2c365-1a0b-426e-978a-232cc0613633"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-build-core==0.9.0\n",
            "  Downloading scikit_build_core-0.9.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-build-core==0.9.0) (1.2.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-build-core==0.9.0) (24.1)\n",
            "Collecting pathspec>=0.10.1 (from scikit-build-core==0.9.0)\n",
            "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tomli>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-build-core==0.9.0) (2.0.1)\n",
            "Downloading scikit_build_core-0.9.0-py3-none-any.whl (151 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/151.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.4/151.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: pathspec, scikit-build-core\n",
            "Successfully installed pathspec-0.12.1 scikit-build-core-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=61\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.62 --force-reinstall --upgrade --no-cache-dir --verbose --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcK82r3PcBqI",
        "outputId": "58f31baa-6e9f-4b33-85a7-7fa2ec8ef5b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 24.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting llama-cpp-python==0.2.62\n",
            "  Downloading llama_cpp_python-0.2.62.tar.gz (37.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.5/37.5 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Running command Preparing metadata (pyproject.toml)\n",
            "  *** scikit-build-core 0.9.0 using CMake 3.30.2 (metadata_wheel)\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.2.62)\n",
            "  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl.metadata\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python==0.2.62)\n",
            "  Obtaining dependency information for numpy>=1.20.0 from https://files.pythonhosted.org/packages/7d/4b/a509d346fffede6120cc17610cc500819417ee9c3da7f08d9aaf15cab2a3/numpy-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading numpy-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.2.62)\n",
            "  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting jinja2>=2.11.3 (from llama-cpp-python==0.2.62)\n",
            "  Obtaining dependency information for jinja2>=2.11.3 from https://files.pythonhosted.org/packages/31/80/3a54838c3fb461f6fec263ebf3a3a41771bd05190238de3486aae8540c36/jinja2-3.1.4-py3-none-any.whl.metadata\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python==0.2.62)\n",
            "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/7c/52/2b1b570f6b8b803cef5ac28fdf78c0da318916c7d2fe9402a84d591b394c/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m199.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m313.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m161.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
            "  *** scikit-build-core 0.9.0 using CMake 3.30.2 (wheel)\n",
            "  *** Configuring CMake...\n",
            "  loading initial cache file /tmp/tmpmpcm1x4v/build/CMakeInit.txt\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:397 (message):\n",
            "    LLAMA_CUBLAS is deprecated and will be removed in the future.\n",
            "\n",
            "    Use LLAMA_CUDA instead\n",
            "\n",
            "\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.2.140\")\n",
            "  -- CUDA found\n",
            "  -- The CUDA compiler identification is NVIDIA 12.2.140\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- Using CUDA architectures: 61\n",
            "  -- CUDA host compiler is GNU 11.4.0\n",
            "\n",
            "  -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  CMake Warning (dev) at CMakeLists.txt:26 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  CMake Warning (dev) at CMakeLists.txt:35 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  -- Configuring done (3.2s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/tmpmpcm1x4v/build\n",
            "  *** Building project with Unix Makefiles...\n",
            "  Change Dir: '/tmp/tmpmpcm1x4v/build'\n",
            "\n",
            "  Run Build Command(s): /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E env VERBOSE=1 /usr/bin/gmake -f Makefile\n",
            "  /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -S/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df -B/tmp/tmpmpcm1x4v/build --check-build-system CMakeFiles/Makefile.cmake 0\n",
            "  /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_progress_start /tmp/tmpmpcm1x4v/build/CMakeFiles /tmp/tmpmpcm1x4v/build//CMakeFiles/progress.marks\n",
            "  /usr/bin/gmake  -f CMakeFiles/Makefile2 all\n",
            "  gmake[1]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml.dir/build.make vendor/llama.cpp/CMakeFiles/ggml.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  cd /tmp/tmpmpcm1x4v/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp /tmp/tmpmpcm1x4v/build /tmp/tmpmpcm1x4v/build/vendor/llama.cpp /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/CMakeFiles/ggml.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml.dir/build.make vendor/llama.cpp/CMakeFiles/ggml.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [  1%] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF CMakeFiles/ggml.dir/ggml.c.o.d -o CMakeFiles/ggml.dir/ggml.c.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml.c\n",
            "  [  3%] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o CMakeFiles/ggml.dir/ggml-alloc.c.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-alloc.c\n",
            "  [  5%] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF CMakeFiles/ggml.dir/ggml-backend.c.o.d -o CMakeFiles/ggml.dir/ggml-backend.c.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-backend.c\n",
            "  [  7%] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF CMakeFiles/ggml.dir/ggml-quants.c.o.d -o CMakeFiles/ggml.dir/ggml-quants.c.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-quants.c\n",
            "  [  9%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/acc.cu -o CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\n",
            "  [ 11%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/alibi.cu -o CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\n",
            "  [ 13%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/arange.cu -o CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\n",
            "  [ 15%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/argsort.cu -o CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\n",
            "  [ 16%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/binbcast.cu -o CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\n",
            "  [ 18%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/clamp.cu -o CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\n",
            "  [ 20%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/concat.cu -o CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\n",
            "  [ 22%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/convert.cu -o CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\n",
            "  [ 24%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/cpy.cu -o CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\n",
            "  [ 26%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/diagmask.cu -o CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\n",
            "  [ 28%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/dmmv.cu -o CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\n",
            "  [ 30%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/getrows.cu -o CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\n",
            "  [ 32%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/im2col.cu -o CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\n",
            "  [ 33%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/mmq.cu -o CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\n",
            "  [ 35%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/mmvq.cu -o CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\n",
            "  [ 37%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/norm.cu -o CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\n",
            "  [ 39%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/pad.cu -o CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\n",
            "  [ 41%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/pool2d.cu -o CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\n",
            "  [ 43%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/quantize.cu -o CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\n",
            "  [ 45%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/rope.cu -o CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\n",
            "  [ 47%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/scale.cu -o CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\n",
            "  [ 49%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/softmax.cu -o CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\n",
            "  [ 50%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/sumrows.cu -o CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\n",
            "  [ 52%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/tsembd.cu -o CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\n",
            "  [ 54%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/unary.cu -o CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\n",
            "  [ 56%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda/upscale.cu -o CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\n",
            "  [ 58%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/ggml-cuda.cu -o CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 58%] Built target ggml\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml_static.dir/build.make vendor/llama.cpp/CMakeFiles/ggml_static.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  cd /tmp/tmpmpcm1x4v/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp /tmp/tmpmpcm1x4v/build /tmp/tmpmpcm1x4v/build/vendor/llama.cpp /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/CMakeFiles/ggml_static.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml_static.dir/build.make vendor/llama.cpp/CMakeFiles/ggml_static.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 60%] Linking CUDA static library libggml_static.a\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -P CMakeFiles/ggml_static.dir/cmake_clean_target.cmake\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/ggml_static.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libggml_static.a CMakeFiles/ggml.dir/ggml.c.o \"CMakeFiles/ggml.dir/ggml-alloc.c.o\" \"CMakeFiles/ggml.dir/ggml-backend.c.o\" \"CMakeFiles/ggml.dir/ggml-quants.c.o\" \"CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda.cu.o\"\n",
            "  /usr/bin/ranlib libggml_static.a\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 60%] Built target ggml_static\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml_shared.dir/build.make vendor/llama.cpp/CMakeFiles/ggml_shared.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  cd /tmp/tmpmpcm1x4v/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp /tmp/tmpmpcm1x4v/build /tmp/tmpmpcm1x4v/build/vendor/llama.cpp /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/CMakeFiles/ggml_shared.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml_shared.dir/build.make vendor/llama.cpp/CMakeFiles/ggml_shared.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 62%] Linking CUDA shared library libggml_shared.so\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/ggml_shared.dir/link.txt --verbose=1\n",
            "  /usr/bin/g++ -fPIC -shared -Wl,-soname,libggml_shared.so -o libggml_shared.so @CMakeFiles/ggml_shared.dir/objects1.rsp @CMakeFiles/ggml_shared.dir/linkLibs.rsp -L\"/usr/local/cuda/targets/x86_64-linux/lib/stubs\" -L\"/usr/local/cuda/targets/x86_64-linux/lib\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 62%] Built target ggml_shared\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/llama.dir/build.make vendor/llama.cpp/CMakeFiles/llama.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  cd /tmp/tmpmpcm1x4v/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp /tmp/tmpmpcm1x4v/build /tmp/tmpmpcm1x4v/build/vendor/llama.cpp /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/CMakeFiles/llama.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/llama.dir/build.make vendor/llama.cpp/CMakeFiles/llama.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 64%] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF CMakeFiles/llama.dir/llama.cpp.o.d -o CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/llama.cpp\n",
            "  [ 66%] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o -MF CMakeFiles/llama.dir/unicode.cpp.o.d -o CMakeFiles/llama.dir/unicode.cpp.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/unicode.cpp\n",
            "  [ 67%] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -MF CMakeFiles/llama.dir/unicode-data.cpp.o.d -o CMakeFiles/llama.dir/unicode-data.cpp.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/unicode-data.cpp\n",
            "  [ 69%] Linking CXX shared library libllama.so\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llama.dir/link.txt --verbose=1\n",
            "  /usr/bin/c++ -fPIC -O3 -DNDEBUG -shared -Wl,-soname,libllama.so -o libllama.so CMakeFiles/llama.dir/llama.cpp.o CMakeFiles/llama.dir/unicode.cpp.o \"CMakeFiles/llama.dir/unicode-data.cpp.o\" CMakeFiles/ggml.dir/ggml.c.o \"CMakeFiles/ggml.dir/ggml-alloc.c.o\" \"CMakeFiles/ggml.dir/ggml-backend.c.o\" \"CMakeFiles/ggml.dir/ggml-quants.c.o\" \"CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda.cu.o\"   -L/usr/local/cuda/targets/x86_64-linux/lib  -Wl,-rpath,/usr/local/cuda-12.2/targets/x86_64-linux/lib: /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a -ldl /usr/lib/x86_64-linux-gnu/librt.a -lcudadevrt -lcudart_static -lrt -lpthread -ldl\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 69%] Built target llama\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/common/CMakeFiles/build_info.dir/build.make vendor/llama.cpp/common/CMakeFiles/build_info.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  cd /tmp/tmpmpcm1x4v/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common /tmp/tmpmpcm1x4v/build /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/common /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/common/CMakeFiles/build_info.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/common/CMakeFiles/build_info.dir/build.make vendor/llama.cpp/common/CMakeFiles/build_info.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 71%] Building CXX object vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600  -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF CMakeFiles/build_info.dir/build-info.cpp.o.d -o CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/build-info.cpp\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 73%] Built target build_info\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/common/CMakeFiles/common.dir/build.make vendor/llama.cpp/common/CMakeFiles/common.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  cd /tmp/tmpmpcm1x4v/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common /tmp/tmpmpcm1x4v/build /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/common /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/common/CMakeFiles/common.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/common/CMakeFiles/common.dir/build.make vendor/llama.cpp/common/CMakeFiles/common.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 75%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/. -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF CMakeFiles/common.dir/common.cpp.o.d -o CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/common.cpp\n",
            "  [ 77%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/. -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF CMakeFiles/common.dir/sampling.cpp.o.d -o CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/sampling.cpp\n",
            "  [ 79%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/. -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF CMakeFiles/common.dir/console.cpp.o.d -o CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/console.cpp\n",
            "  [ 81%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/. -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF CMakeFiles/common.dir/grammar-parser.cpp.o.d -o CMakeFiles/common.dir/grammar-parser.cpp.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/grammar-parser.cpp\n",
            "  [ 83%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/. -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -MF CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d -o CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/json-schema-to-grammar.cpp\n",
            "  [ 84%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/. -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF CMakeFiles/common.dir/train.cpp.o.d -o CMakeFiles/common.dir/train.cpp.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/train.cpp\n",
            "  [ 86%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/. -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -MF CMakeFiles/common.dir/ngram-cache.cpp.o.d -o CMakeFiles/common.dir/ngram-cache.cpp.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/ngram-cache.cpp\n",
            "  [ 88%] Linking CXX static library libcommon.a\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/common && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -P CMakeFiles/common.dir/cmake_clean_target.cmake\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/common && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/common.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libcommon.a CMakeFiles/common.dir/common.cpp.o CMakeFiles/common.dir/sampling.cpp.o CMakeFiles/common.dir/console.cpp.o \"CMakeFiles/common.dir/grammar-parser.cpp.o\" \"CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\" CMakeFiles/common.dir/train.cpp.o \"CMakeFiles/common.dir/ngram-cache.cpp.o\" \"CMakeFiles/build_info.dir/build-info.cpp.o\"\n",
            "  /usr/bin/ranlib libcommon.a\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 88%] Built target common\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  cd /tmp/tmpmpcm1x4v/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava /tmp/tmpmpcm1x4v/build /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/examples/llava /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 90%] Building CXX object vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/examples/llava && /usr/bin/c++ -DGGML_USE_CUBLAS -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF CMakeFiles/llava.dir/llava.cpp.o.d -o CMakeFiles/llava.dir/llava.cpp.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava/llava.cpp\n",
            "  [ 92%] Building CXX object vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/examples/llava && /usr/bin/c++ -DGGML_USE_CUBLAS -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF CMakeFiles/llava.dir/clip.cpp.o.d -o CMakeFiles/llava.dir/clip.cpp.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava/clip.cpp\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 92%] Built target llava\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava_static.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava_static.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  cd /tmp/tmpmpcm1x4v/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava /tmp/tmpmpcm1x4v/build /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/examples/llava /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/examples/llava/CMakeFiles/llava_static.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava_static.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava_static.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 94%] Linking CXX static library libllava_static.a\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/examples/llava && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -P CMakeFiles/llava_static.dir/cmake_clean_target.cmake\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/examples/llava && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llava_static.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libllava_static.a CMakeFiles/llava.dir/llava.cpp.o CMakeFiles/llava.dir/clip.cpp.o\n",
            "  /usr/bin/ranlib libllava_static.a\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 94%] Built target llava_static\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava_shared.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava_shared.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  cd /tmp/tmpmpcm1x4v/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava /tmp/tmpmpcm1x4v/build /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/examples/llava /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/examples/llava/CMakeFiles/llava_shared.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava_shared.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava_shared.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 96%] Linking CXX shared library libllava.so\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/examples/llava && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llava_shared.dir/link.txt --verbose=1\n",
            "  /usr/bin/c++ -fPIC -O3 -DNDEBUG -shared -Wl,-soname,libllava.so -o libllava.so CMakeFiles/llava.dir/llava.cpp.o CMakeFiles/llava.dir/clip.cpp.o ../../CMakeFiles/ggml.dir/ggml.c.o \"../../CMakeFiles/ggml.dir/ggml-alloc.c.o\" \"../../CMakeFiles/ggml.dir/ggml-backend.c.o\" \"../../CMakeFiles/ggml.dir/ggml-quants.c.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda.cu.o\"  -Wl,-rpath,/tmp/tmpmpcm1x4v/build/vendor/llama.cpp:/usr/local/cuda-12.2/targets/x86_64-linux/lib: ../../libllama.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so -ldl /usr/lib/x86_64-linux-gnu/librt.a\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 96%] Built target llava_shared\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  cd /tmp/tmpmpcm1x4v/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava /tmp/tmpmpcm1x4v/build /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/examples/llava /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [ 98%] Building CXX object vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/examples/llava && /usr/bin/c++ -DGGML_USE_CUBLAS -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/common/. -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/. -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava/../../common -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -MF CMakeFiles/llava-cli.dir/llava-cli.cpp.o.d -o CMakeFiles/llava-cli.dir/llava-cli.cpp.o -c /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/vendor/llama.cpp/examples/llava/llava-cli.cpp\n",
            "  [100%] Linking CXX executable llava-cli\n",
            "  cd /tmp/tmpmpcm1x4v/build/vendor/llama.cpp/examples/llava && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llava-cli.dir/link.txt --verbose=1\n",
            "  /usr/bin/c++ -O3 -DNDEBUG \"CMakeFiles/llava-cli.dir/llava-cli.cpp.o\" CMakeFiles/llava.dir/llava.cpp.o CMakeFiles/llava.dir/clip.cpp.o -o llava-cli  -Wl,-rpath,/tmp/tmpmpcm1x4v/build/vendor/llama.cpp:/usr/local/cuda-12.2/targets/x86_64-linux/lib: ../../common/libcommon.a ../../libllama.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so -ldl /usr/lib/x86_64-linux-gnu/librt.a\n",
            "  gmake[2]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  [100%] Built target llava-cli\n",
            "  gmake[1]: Leaving directory '/tmp/tmpmpcm1x4v/build'\n",
            "  /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_progress_start /tmp/tmpmpcm1x4v/build/CMakeFiles 0\n",
            "\n",
            "  *** Installing project into wheel...\n",
            "  -- Install configuration: \"Release\"\n",
            "  -- Installing: /tmp/tmpmpcm1x4v/wheel/platlib/lib/libggml_shared.so\n",
            "  -- Installing: /tmp/tmpmpcm1x4v/wheel/platlib/lib/cmake/Llama/LlamaConfig.cmake\n",
            "  -- Installing: /tmp/tmpmpcm1x4v/wheel/platlib/lib/cmake/Llama/LlamaConfigVersion.cmake\n",
            "  -- Installing: /tmp/tmpmpcm1x4v/wheel/platlib/include/ggml.h\n",
            "  -- Installing: /tmp/tmpmpcm1x4v/wheel/platlib/include/ggml-alloc.h\n",
            "  -- Installing: /tmp/tmpmpcm1x4v/wheel/platlib/include/ggml-backend.h\n",
            "  -- Installing: /tmp/tmpmpcm1x4v/wheel/platlib/include/ggml-cuda.h\n",
            "  -- Installing: /tmp/tmpmpcm1x4v/wheel/platlib/lib/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpmpcm1x4v/wheel/platlib/lib/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/tmpmpcm1x4v/wheel/platlib/include/llama.h\n",
            "  -- Installing: /tmp/tmpmpcm1x4v/wheel/platlib/bin/convert.py\n",
            "  -- Installing: /tmp/tmpmpcm1x4v/wheel/platlib/bin/convert-lora-to-ggml.py\n",
            "  -- Installing: /tmp/tmpmpcm1x4v/wheel/platlib/llama_cpp/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpmpcm1x4v/wheel/platlib/llama_cpp/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/llama_cpp/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/llama_cpp/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/tmpmpcm1x4v/wheel/platlib/lib/libllava.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpmpcm1x4v/wheel/platlib/lib/libllava.so\" to \"\"\n",
            "  -- Installing: /tmp/tmpmpcm1x4v/wheel/platlib/bin/llava-cli\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpmpcm1x4v/wheel/platlib/bin/llava-cli\" to \"\"\n",
            "  -- Installing: /tmp/tmpmpcm1x4v/wheel/platlib/llama_cpp/libllava.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpmpcm1x4v/wheel/platlib/llama_cpp/libllava.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/llama_cpp/libllava.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-dqs9w0w4/llama-cpp-python_b8c0e2e4acd445879c4e87f9a32364df/llama_cpp/libllava.so\" to \"\"\n",
            "  *** Making wheel...\n",
            "  *** Created llama_cpp_python-0.2.62-cp310-cp310-linux_x86_64.whl...\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.62-cp310-cp310-linux_x86_64.whl size=18239321 sha256=43b553c1797620e3d7f0c3e9758d820f788fdbeb6f79b6b9246475db538acbbe\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1oezndoi/wheels/c0/81/de/d4cc8f152d89865379dbf28ca672358c667192ee55deaca7cb\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.12.2.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.1\n",
            "    Uninstalling numpy-2.1.1:\n",
            "      Removing file or directory /usr/local/bin/f2py\n",
            "      Removing file or directory /usr/local/bin/numpy-config\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-2.1.1.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n",
            "      Successfully uninstalled numpy-2.1.1\n",
            "  changing mode of /usr/local/bin/f2py to 755\n",
            "  changing mode of /usr/local/bin/numpy-config to 755\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/MarkupSafe-2.1.5.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/markupsafe/\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache-5.6.3.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache/\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.4\n",
            "    Uninstalling Jinja2-3.1.4:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/jinja2-3.1.4.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/jinja2/\n",
            "      Successfully uninstalled Jinja2-3.1.4\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.1.78\n",
            "    Uninstalling llama_cpp_python-0.1.78:\n",
            "      Removing file or directory /usr/local/bin/__pycache__/convert-lora-to-ggml.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/bin/__pycache__/convert.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/bin/convert-lora-to-ggml.py\n",
            "      Removing file or directory /usr/local/bin/convert.py\n",
            "      Removing file or directory /usr/local/lib/libggml_shared.so\n",
            "      Removing file or directory /usr/local/lib/libllama.so\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/llama_cpp/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/llama_cpp_python-0.1.78.dist-info/\n",
            "      Successfully uninstalled llama_cpp_python-0.1.78\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "accelerate 0.33.0 requires numpy<2.0.0,>=1.17, but you have numpy 2.1.1 which is incompatible.\n",
            "arviz 0.18.0 requires numpy<2.0,>=1.23.0, but you have numpy 2.1.1 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.1 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.1.1 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.1 which is incompatible.\n",
            "ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.1.1 which is incompatible.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.4.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.1 which is incompatible.\n",
            "pandas 2.1.4 requires numpy<2,>=1.22.4; python_version < \"3.11\", but you have numpy 2.1.1 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.1 which is incompatible.\n",
            "scikit-learn 1.3.2 requires numpy<2.0,>=1.17.3, but you have numpy 2.1.1 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.1.1 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.62 numpy-2.1.1 typing-extensions-4.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avbnQyUcgDto",
        "outputId": "75a62e6f-94eb-4d63-b8f6-49e0b1f246e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-05 18:15:22--  https://huggingface.co/fhai50032/BeagleLake-7B-GGUF/resolve/main/BeagleLake-7B.Q8_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.33.28.94, 13.33.28.87, 13.33.28.54, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.33.28.94|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/f0/05/f005d362adc1529258f3fc67c74b31622c8d74747cc9188de7811a7dcf1d7339/b629064f3f5562a7746f2f83602c7ab952b8c7ade62d8335cba828e0fd7df0bc?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27BeagleLake-7B.Q8_0.gguf%3B+filename%3D%22BeagleLake-7B.Q8_0.gguf%22%3B&Expires=1725819322&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNTgxOTMyMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2YwLzA1L2YwMDVkMzYyYWRjMTUyOTI1OGYzZmM2N2M3NGIzMTYyMmM4ZDc0NzQ3Y2M5MTg4ZGU3ODExYTdkY2YxZDczMzkvYjYyOTA2NGYzZjU1NjJhNzc0NmYyZjgzNjAyYzdhYjk1MmI4YzdhZGU2MmQ4MzM1Y2JhODI4ZTBmZDdkZjBiYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=VF2yE%7EmQqZ-w1CmYb2ayemIiC7itAkt9bp7x%7EvTWNMavriO784dRIUEaOcmrTAB-4BEa-J1LdWGyB4RaSqGoGbnjA9Nhh5pLEZ3c6cml6nId94bHp0T3wryGemoE0GwEASjKcOfSlhpc0iDm6BPpX3lEE9OKX64wMCBleNuv65pg8BlW-bUY3I6tbb2Y7R0jjfWd%7EDGeHQg1W0X%7E3JmWN7ft4AOZwMbDAFYibueAi%7EX0ys5E1cHNMKiRq1CemyhX9ctYoKYmHJ7s8V%7E2rzm4Qm9wyugmOYbO5PuRFFQY3ocPvMfBw-plHoADLXqAzZMUMA3OUAGTzslu4LjnntlxaQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2024-09-05 18:15:22--  https://cdn-lfs-us-1.huggingface.co/repos/f0/05/f005d362adc1529258f3fc67c74b31622c8d74747cc9188de7811a7dcf1d7339/b629064f3f5562a7746f2f83602c7ab952b8c7ade62d8335cba828e0fd7df0bc?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27BeagleLake-7B.Q8_0.gguf%3B+filename%3D%22BeagleLake-7B.Q8_0.gguf%22%3B&Expires=1725819322&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNTgxOTMyMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2YwLzA1L2YwMDVkMzYyYWRjMTUyOTI1OGYzZmM2N2M3NGIzMTYyMmM4ZDc0NzQ3Y2M5MTg4ZGU3ODExYTdkY2YxZDczMzkvYjYyOTA2NGYzZjU1NjJhNzc0NmYyZjgzNjAyYzdhYjk1MmI4YzdhZGU2MmQ4MzM1Y2JhODI4ZTBmZDdkZjBiYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=VF2yE%7EmQqZ-w1CmYb2ayemIiC7itAkt9bp7x%7EvTWNMavriO784dRIUEaOcmrTAB-4BEa-J1LdWGyB4RaSqGoGbnjA9Nhh5pLEZ3c6cml6nId94bHp0T3wryGemoE0GwEASjKcOfSlhpc0iDm6BPpX3lEE9OKX64wMCBleNuv65pg8BlW-bUY3I6tbb2Y7R0jjfWd%7EDGeHQg1W0X%7E3JmWN7ft4AOZwMbDAFYibueAi%7EX0ys5E1cHNMKiRq1CemyhX9ctYoKYmHJ7s8V%7E2rzm4Qm9wyugmOYbO5PuRFFQY3ocPvMfBw-plHoADLXqAzZMUMA3OUAGTzslu4LjnntlxaQ__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 3.165.102.95, 3.165.102.80, 3.165.102.112, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|3.165.102.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7695857536 (7.2G) [binary/octet-stream]\n",
            "Saving to: ‘BeagleLake-7B.Q8_0.gguf’\n",
            "\n",
            "BeagleLake-7B.Q8_0. 100%[===================>]   7.17G  23.0MB/s    in 5m 19s  \n",
            "\n",
            "2024-09-05 18:20:42 (23.0 MB/s) - ‘BeagleLake-7B.Q8_0.gguf’ saved [7695857536/7695857536]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/fhai50032/BeagleLake-7B-GGUF/resolve/main/BeagleLake-7B.Q8_0.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8LuZlsxz0qp9"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok --quiet\n",
        "!pip install starlette-context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1qdOgJuRU4U3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pyngrok import ngrok, conf\n",
        "os.environ[\"NGROK\"] = \"2bWuFf5hFuuv761QzAJBnNAiAq9_4tCnQseBH6hQBCYrQrkij\"\n",
        "conf.get_default().auth_token = os.environ[\"NGROK\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oEXMcvWj0GU7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "port=911\n",
        "config_content = {\n",
        "    \"host\": \"0.0.0.0\",\n",
        "    \"port\": f\"{port}\",\n",
        "    \"models\": [\n",
        "        {\n",
        "            \"model\": \"/content/BeagleLake-7B.Q8_0.gguf\",\n",
        "            \"model_alias\": \"gpt-3.5-turbo\",\n",
        "            \"chat_format\": \"chatml\",\n",
        "            \"n_gpu_layers\": -1,\n",
        "            \"offload_kqv\": True,\n",
        "            \"n_threads\": 12,\n",
        "            \"n_batch\": 512,\n",
        "            \"n_ctx\": 20006\n",
        "        }]\n",
        "}\n",
        "\n",
        "with open('config.json', 'w') as json_file:\n",
        "    json.dump(config_content, json_file, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RskKy7Uk1Efd"
      },
      "outputs": [],
      "source": [
        "!python3 -m llama_cpp.server --config_file /content/config.json > server.log 2>&1 &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkXioHR-X1uX",
        "outputId": "2ec61515-816b-4f2e-84f2-7c17bb28a358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok tunnel \"https://ee01-34-143-169-105.ngrok-free.app\" -> \"http://127.0.0.1:911\"\n"
          ]
        }
      ],
      "source": [
        "public_url = ngrok.connect(f\"{port}\").public_url\n",
        "print(\"ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB-YKzr82SG8"
      },
      "outputs": [],
      "source": [
        "!pkill uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9P977GnIMpGr"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "import torch\n",
        "# if llm:\n",
        "#   del llm\n",
        "# llm=None\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}